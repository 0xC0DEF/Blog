{% extends "base.html" %}
{% block content %}
<h2>인공지능을 공부하는 이유</h2>
<ol>
	<li>
		<p>경진대회 알고리즘을 공부하면서, (Deterministic한) 알고리즘의 다양한 활용성 뿐만 아니라 한계 또한 느낄 수 있었다. 세상의 많은 문제들은 Deterministic하게 풀 수 없는 문제들로
			가득하다. 계산복잡도상 어려운 not P 문제들과 빅데이터환경, 인간은 쉽게 할 수 있지만 알고리즘으로는 정의조차 명확히 되지 않는 작업들(Image or Speech Recognition 등)이 얼마나
			많은가? 그래서 알고리즘을 넘어 Combinatorial Optimization, (Machine|Deep|Reinforcement) Learning등을 찾아보게 되었다. RL이 재밌어서 RL위주로
			공부하고
			있다.</p>
	</li>
</ol>
<h3>추천 도서</h3>
<ul>
	<li>Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville 저)</li>
	<li>파이썬과 케라스로 배우는 강화학습 (이웅원, 양혁렬, 김건우, 이영무, 이의령 저)</li>
	<li>바닥부터 배우는 강화학습 (노승은 저)</li>
	<li>강화학습 첫걸음 (아서 줄리아니 저)</li>
	<li>Deep Reinforcement Learning in Action (Alexander Zai, Brandon Brown 저)</li>
</ul>
<h3>온라인 자료</h3>
<p>DeepMind의 논문들 대부분이 arxiv에 올라와있으니 꼭 읽어보자. 내가 지금까지 보았던 논문들중 최고로 간결하고 친절하게 설명해준다. 심지어 짧다!!!</p>
<ul>
	<a href="https://arxiv.org/pdf/1312.5602.pdf">DQN</a><br />
	<a href="https://arxiv.org/pdf/1509.06461.pdf">Double DQN</a><br />
	<a href="https://arxiv.org/pdf/1511.06581.pdf">Dueling DQN</a><br />
	<a href="https://arxiv.org/pdf/1511.05952.pdf">Prioritized Experience Replay Memory</a><br />
	<a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">AlphaGo Zero</a><br />
	<a href="https://arxiv.org/pdf/1911.08265.pdf">Mu Zero</a><br />
	<a href="https://openai.com/">open.ai</a><br />
	<a href="https://www.fast.ai">fast.ai</a><br />
	<a href="https://developers.google.com/machine-learning/crash-course">구글 머신러닝 온라인강의</a><br />
	<a href="http://hunkim.github.io/ml/">모두를 위한 머신러닝/딥러닝</a><br />
	<a href="https://subinium.github.io">수비니움</a><br />
</ul>
{% endblock %}